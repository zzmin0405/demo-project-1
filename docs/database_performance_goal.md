# 데이터베이스 성능 목표 제안서 (Performance Goal) - 수정안

## 1. 개요
*   **프로젝트명:** AI-Meet
*   **주제:** **"인덱싱(Indexing)을 활용한 대용량 회의 검색 성능 최적화"**
*   **핵심 내용:** 텍스트 검색(Search) 시 발생하는 조회 지연을 인덱스 생성으로 해결.

## 2. 요소기술 인정 기준 충족 여부
*   **[필수] 테이블 4개 이상:** `MeetingRoom`, `User`, `Participant`, `Link` 등 8개 운용 중.
*   **[필수] 100개 이상 Tuple:** 현재 DB에 충분한 양의 데이터 적재 (또는 더미 데이터로 증명 가능).
*   **[필수] 외래키 & 3정규형:** Prisma Schema를 통해 이미 완벽하게 구축됨.

## 3. 핵심 성능 목표 (간소화 버전)

**"회의 제목 검색 쿼리 응답 속도를 0.01초(10ms) 이내로 유지"**

### 상세 시나리오
1.  **상황:** 사용자가 회의 목록에서 특정 키워드(예: "프로젝트 진행")로 회의를 검색함.
2.  **데이터:** `MeetingRoom` 테이블에 데이터가 쌓여있을 때, 기본 조회(`Full Table Scan`)는 데이터가 많아질수록 느려짐.
3.  **최적화:** `title` 컬럼에 **Index**를 생성하여 조회 방식을 `Index Scan`으로 변경.

### 선정 이유 (왜 이것을 선택해야 하는가?)
1.  **가장 간단함:** 복잡한 JOIN 튜닝 없이, **"인덱스 생성(CREATE INDEX)"** 한 줄로 드라마틱한 성능 향상을 보여줄 수 있음.
2.  **명확한 비포/애프터:** 인덱스 없을 때(느림) vs 있을 때(빠름)가 확실하게 비교됨.
3.  **실무 적합성:** 검색 기능은 모든 웹 서비스의 필수 기능이며, 성능 이슈가 가장 빈번한 곳임.

## 4. 달성 및 검증 계획 (상세 측정 방법)

**"객관적 데이터 확보를 위해 100회 반복 측정 후 통계적 유의성을 검증합니다."**

### 4-1. 측정 프로토콜
1.  **테스트 환경 고정:** PostgreSQL + Prisma (네트워크/하드웨어 변수 통제)
2.  **반복 실행:** 동일한 `SELECT` 쿼리를 **100회 반복 실행**하여 캐싱 효과 및 일시적 지연(Spike)을 평탄화.
3.  **데이터 기록:** 각 실행(Iteration)마다 **시작(Start) - 종료(End) 타임스탬프**를 정밀 기록 (ms 단위).

### 4-2. 검증 지표 및 목표
*   **평균 응답 시간 (Average):** 100회 실행 평균이 **10ms 이하**인지 확인.
*   **안정성 검증 (Stability):** 최댓값(Max)과 최솟값(Min)을 함께 제하여, 쿼리 성능이 얼마나 안정적인지 증명.

### 4-3. 최종 결과 산출물 예시
| 구분 | 인덱스 미적용 (Before) | 인덱스 적용 (After) | 성능 향상률 |
| :--- | :---: | :---: | :---: |
| 평균 응답 시간 | 150 ms | **5 ms** | **30배 (3000%)** |
| 최악 응답 시간 | 320 ms | 12 ms | - |
